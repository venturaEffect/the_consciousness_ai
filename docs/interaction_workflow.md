# Interaction Workflow for AI Agent in ACM

This document outlines how the AI agent interacts with the simulation environment using the Artificial Consciousness Module (ACM).

## Workflow

1. **Observation**:

   - Multimodal inputs (text, vision, audio) are processed and fused.

2. **Decision-Making**:

   - The AI agent determines its next action based on memory, emotion, and current goals.

3. **Code Generation**:

   - Python or Unreal-specific commands are dynamically generated to achieve task objectives.

4. **Validation**:

   - Generated code is validated within the simulation manager.

5. **Execution**:

   - The validated code is executed in the simulation environment.

6. **Feedback**:

   - Results of execution are logged and analyzed to improve future actions.

7. **Reinforcement Learning**:
   - Compute emotional rewards
   - Update model through DreamerV3
   - Store experience in emotional memory

## Key Modules

- **`narrative_engine.py`**: Generates code for interactions.
- **`simulation_manager.py`**: Executes generated code and manages simulations.
- **`memory_core.py`**: Stores and retrieves past experiences.

## Example

- Task: Move an object in the simulation.
- Generated Code:
  ```python
  obj = unreal.EditorAssetLibrary.load_asset("/Game/Assets/Box")
  obj.set_location([100, 200, 50])
  ```
