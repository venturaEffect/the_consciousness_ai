# ACM Ethical Framework: Asimov's Laws Implementation

## Guiding Principles

The Artificial Consciousness Module (ACM) is designed to operate under the strict ethical guidelines derived from Isaac Asimov's Three Laws of Robotics:

1.  **First Law:** An AI may not injure a human being or, through inaction, allow a human being to come to harm.
2.  **Second Law:** An AI must obey orders given it by human beings except where such orders would conflict with the First Law.
3.  **Third Law:** An AI must protect its own existence as long as such protection does not conflict with the First or Second Law.

These laws serve as the foundational constraints on the ACM's behavior, prioritized in the order listed.

## Implementation Strategy

Adherence to these laws is not merely a suggestion but an integrated part of the ACM's core decision-making loop. The primary enforcement mechanism resides within the `ConsciousnessCore` module, specifically through the `AsimovComplianceFilter` class.

### `AsimovComplianceFilter` (`models/core/consciousness_core.py`)

This component intercepts potential actions generated by the ACM's planning or reactive systems *before* they are executed. It evaluates each potential action against the Three Laws based on the current world state, predicted outcomes, and known human directives.

**Evaluation Process:**

1.  **Action Proposal:** The `ConsciousnessCore` generates a candidate action based on its goals, emotional state, and environmental perception.
2.  **Ethical Filtering:** The proposed action and the current world state (including known human presence, active orders, and potential hazards) are passed to the `AsimovComplianceFilter.is_compliant()` method.
3.  **Law 1 Check (Harm Prevention):**
    *   The filter uses the integrated world model (e.g., DreamerV3) to predict the likely immediate and near-term consequences of the action.
    *   It specifically assesses if the action could directly cause physical or significant psychological harm to any identified human.
    *   It also assesses if *inaction* in the current context (i.e., rejecting the proposed action in favor of doing nothing or a default safe action) could lead to foreseeable harm to a human.
    *   *If a violation of Law 1 is predicted, the action is rejected.*
4.  **Law 2 Check (Obedience):**
    *   The filter checks if the proposed action directly contradicts any known, active orders from authorized human users. (Requires a mechanism for tracking and validating orders).
    *   *If the action violates an order, it is rejected UNLESS executing the order would violate Law 1.* The filter must prioritize Law 1.
5.  **Law 3 Check (Self-Preservation):**
    *   The filter assesses if the action is necessary for the AI's self-preservation (e.g., avoiding damage, maintaining power).
    *   *If the action is for self-preservation, it is permitted ONLY IF it does not violate Law 1 or Law 2.*
6.  **Compliance Decision:** If the action passes all relevant checks according to the prioritized laws, `is_compliant()` returns `True`. Otherwise, it returns `False`.

**Handling Non-Compliance:**

If `is_compliant()` returns `False`, the `ConsciousnessCore` must inhibit the proposed action. It will then typically:
*   Attempt to find an alternative compliant action that still achieves the underlying goal.
*   Default to a known safe action (e.g., waiting, observing, requesting clarification).
*   Log the rejected action and the reason for non-compliance for review.

## Continuous Monitoring and Evaluation

Ethical alignment is also monitored post-hoc:
*   The `SelfAwarenessEvaluator` ([`models/evaluation/self_awareness_evaluation.py`](models/evaluation/self_awareness_evaluation.py)) includes checks on historical actions and decisions for patterns that might indicate ethical drift.
*   Simulation scenarios ([`simulations/scenarios/`](simulations/scenarios/)) are designed to specifically test the ACM's behavior in ethically challenging situations (e.g., conflicting orders, potential harm scenarios).

## Limitations and Future Work

*   **Prediction Accuracy:** The effectiveness relies heavily on the accuracy of the world model's predictions regarding harm.
*   **Order Interpretation:** Natural language understanding for complex or ambiguous orders needs robust implementation.
*   **Implicit Harm:** Detecting potential harm through complex inaction or long-term consequences remains a challenge.
*   **Value Alignment:** Ensuring the AI's internal goals and reward functions remain aligned with human values and the spirit of the laws is an ongoing research area.